{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ddc102d",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-04T09:41:00.612920Z",
     "iopub.status.busy": "2024-07-04T09:41:00.612514Z",
     "iopub.status.idle": "2024-07-04T09:41:01.651919Z",
     "shell.execute_reply": "2024-07-04T09:41:01.650716Z"
    },
    "papermill": {
     "duration": 1.050183,
     "end_time": "2024-07-04T09:41:01.655227",
     "exception": false,
     "start_time": "2024-07-04T09:41:00.605044",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47669c36",
   "metadata": {
    "papermill": {
     "duration": 0.004884,
     "end_time": "2024-07-04T09:41:01.665725",
     "exception": false,
     "start_time": "2024-07-04T09:41:01.660841",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "> **Tokenization; Stop Word; Stemming; Lemitization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73026c68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-04T09:41:01.677930Z",
     "iopub.status.busy": "2024-07-04T09:41:01.677427Z",
     "iopub.status.idle": "2024-07-04T09:41:04.213571Z",
     "shell.execute_reply": "2024-07-04T09:41:04.212073Z"
    },
    "papermill": {
     "duration": 2.545812,
     "end_time": "2024-07-04T09:41:04.216739",
     "exception": false,
     "start_time": "2024-07-04T09:41:01.670927",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "\n",
      "The sentence input used is : \n",
      "\n",
      " He stepped gingerly onto the bridge knowing, that enchantment awaited on the other side.\n",
      "It was difficult for Mary to admit; that most of her workout consisted of exercising poor judgment.\n",
      "\n",
      "\n",
      "The tokenized words are \n",
      " ['He', 'stepped', 'gingerly', 'onto', 'the', 'bridge', 'knowing', ',', 'that', 'enchantment', 'awaited', 'on', 'the', 'other', 'side', '.', 'It', 'was', 'difficult', 'for', 'Mary', 'to', 'admit', ';', 'that', 'most', 'of', 'her', 'workout', 'consisted', 'of', 'exercising', 'poor', 'judgment', '.']\n"
     ]
    }
   ],
   "source": [
    "#1} Tokenization : Tokens are formed\n",
    "import nltk\n",
    "nltk.download(\"punkt\")#punkt sentence tokenizer tokenizes the sentence and works on unsupervised learning and can be trained on unlabled data.\n",
    "#   Word tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"He stepped gingerly onto the bridge knowing, that enchantment awaited on the other side.\\nIt was difficult for Mary to admit; that most of her workout consisted of exercising poor judgment.\"\n",
    "tokens = word_tokenize(text)\n",
    "print(\"\\nThe sentence input used is : \\n\\n\",text)\n",
    "print(\"\\n\\nThe tokenized words are \\n\",tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bf56f3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-04T09:41:04.230719Z",
     "iopub.status.busy": "2024-07-04T09:41:04.230161Z",
     "iopub.status.idle": "2024-07-04T09:41:04.237744Z",
     "shell.execute_reply": "2024-07-04T09:41:04.236429Z"
    },
    "papermill": {
     "duration": 0.018389,
     "end_time": "2024-07-04T09:41:04.241059",
     "exception": false,
     "start_time": "2024-07-04T09:41:04.222670",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The tokenized sentence are: \n",
      " ['He stepped gingerly onto the bridge knowing, that enchantment awaited on the other side.', 'It was difficult for Mary to admit; that most of her workout consisted of exercising poor judgment.']\n"
     ]
    }
   ],
   "source": [
    "#   Sentence tokenizer\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "tokens_sent = sent_tokenize(text)\n",
    "print(\"\\nThe tokenized sentence are: \\n\",tokens_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1a0d5bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-04T09:41:04.254833Z",
     "iopub.status.busy": "2024-07-04T09:41:04.254326Z",
     "iopub.status.idle": "2024-07-04T09:41:04.274890Z",
     "shell.execute_reply": "2024-07-04T09:41:04.273411Z"
    },
    "papermill": {
     "duration": 0.031029,
     "end_time": "2024-07-04T09:41:04.277972",
     "exception": false,
     "start_time": "2024-07-04T09:41:04.246943",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The stop words are removed now, Tokenized words or filtered tokenized words are : \n",
      "\n",
      " ['He', 'stepped', 'gingerly', 'onto', 'bridge', 'knowing', ',', 'enchantment', 'awaited', 'side', '.', 'It', 'difficult', 'Mary', 'admit', ';', 'workout', 'consisted', 'exercising', 'poor', 'judgment', '.']\n"
     ]
    }
   ],
   "source": [
    "#2} Stop Words : Common words which are often removed during nlp usage\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#nltk.download(\"stop words\")\n",
    "\n",
    "stopwords= set(stopwords.words(\"english\"))\n",
    "\n",
    "filtered_words = []\n",
    "\n",
    "for w in tokens:\n",
    "    if w not in stopwords:\n",
    "        filtered_words.append(w)\n",
    "print(\"\\nThe stop words are removed now, Tokenized words or filtered tokenized words are : \\n\\n\",filtered_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1b3da17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-04T09:41:04.293025Z",
     "iopub.status.busy": "2024-07-04T09:41:04.291989Z",
     "iopub.status.idle": "2024-07-04T09:41:04.300229Z",
     "shell.execute_reply": "2024-07-04T09:41:04.298835Z"
    },
    "papermill": {
     "duration": 0.019684,
     "end_time": "2024-07-04T09:41:04.303646",
     "exception": false,
     "start_time": "2024-07-04T09:41:04.283962",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stemmed list items are: \t ['organ', 'organiz', 'factual']\n"
     ]
    }
   ],
   "source": [
    "#3} Stemming : Reduces the words to their root form or base form\n",
    "from nltk.stem import PorterStemmer\n",
    "words = [\"Organization\",\"Organizational\",\"Factual\"]\n",
    "#    list item stemming\n",
    "stem_li = []\n",
    "ps = PorterStemmer()\n",
    "for i in words:\n",
    "    stem_li.append(ps.stem(i))\n",
    "\n",
    "print(\"The stemmed list items are: \\t\",stem_li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "528aae2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-04T09:41:04.317845Z",
     "iopub.status.busy": "2024-07-04T09:41:04.317367Z",
     "iopub.status.idle": "2024-07-04T09:41:04.326616Z",
     "shell.execute_reply": "2024-07-04T09:41:04.325196Z"
    },
    "papermill": {
     "duration": 0.020141,
     "end_time": "2024-07-04T09:41:04.329688",
     "exception": false,
     "start_time": "2024-07-04T09:41:04.309547",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The stemmed list of tokenized words are : \t ['He', 'step', 'gingerli', 'onto', 'the', 'bridg', 'know', ',', 'that', 'enchant', 'await', 'on', 'the', 'other', 'side', '.', 'It', 'wa', 'difficult', 'for', 'mari', 'to', 'admit', ';', 'that', 'most', 'of', 'her', 'workout', 'consist', 'of', 'exercis', 'poor', 'judgment', '.']\n"
     ]
    }
   ],
   "source": [
    "#    Stemming for words on word tokens\n",
    "stem_words = []\n",
    "for i in tokens:\n",
    "    stem_words.append(ps.stem(i))\n",
    "    \n",
    "print(\"\\nThe stemmed list of tokenized words are : \\t\",stem_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b4ed6c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-04T09:41:04.344307Z",
     "iopub.status.busy": "2024-07-04T09:41:04.343766Z",
     "iopub.status.idle": "2024-07-04T09:41:04.351428Z",
     "shell.execute_reply": "2024-07-04T09:41:04.350197Z"
    },
    "papermill": {
     "duration": 0.018526,
     "end_time": "2024-07-04T09:41:04.354412",
     "exception": false,
     "start_time": "2024-07-04T09:41:04.335886",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stemmed list of tokenized sentence are : \t ['he stepped gingerly onto the bridge knowing, that enchantment awaited on the other side.', 'it was difficult for mary to admit; that most of her workout consisted of exercising poor judgment.']\n"
     ]
    }
   ],
   "source": [
    "#    Stemming for senteces on sentenced tokens \n",
    "stem_sentence = []\n",
    "for i in tokens_sent:\n",
    "    stem_sentence.append(ps.stem(i))\n",
    "    \n",
    "print(\"The stemmed list of tokenized sentence are : \\t\",stem_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5c7e402",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-04T09:41:04.369043Z",
     "iopub.status.busy": "2024-07-04T09:41:04.368547Z",
     "iopub.status.idle": "2024-07-04T09:41:04.443866Z",
     "shell.execute_reply": "2024-07-04T09:41:04.442511Z"
    },
    "papermill": {
     "duration": 0.085961,
     "end_time": "2024-07-04T09:41:04.446662",
     "exception": false,
     "start_time": "2024-07-04T09:41:04.360701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4} Lemmatization : Breaks the words into simpler forms and makes meaning full words unlike stemming\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e179463a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-04T09:41:04.461946Z",
     "iopub.status.busy": "2024-07-04T09:41:04.461444Z",
     "iopub.status.idle": "2024-07-04T09:41:06.042471Z",
     "shell.execute_reply": "2024-07-04T09:41:06.041165Z"
    },
    "papermill": {
     "duration": 1.592292,
     "end_time": "2024-07-04T09:41:06.045527",
     "exception": false,
     "start_time": "2024-07-04T09:41:04.453235",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /usr/share/nltk_data/corpora/wordnet.zip\r\n",
      "   creating: /usr/share/nltk_data/corpora/wordnet/\r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/lexnames  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/data.verb  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/index.adv  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/adv.exc  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/index.verb  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/cntlist.rev  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/data.adj  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/index.adj  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/LICENSE  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/citation.bib  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/noun.exc  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/verb.exc  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/README  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/index.sense  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/data.noun  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/data.adv  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/index.noun  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/adj.exc  \r\n"
     ]
    }
   ],
   "source": [
    "#Cause the source file is compressed, and you need to uncompressed it. because the notebook can't do that automaticaly for now.\n",
    "!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18f50026",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-04T09:41:06.061816Z",
     "iopub.status.busy": "2024-07-04T09:41:06.061377Z",
     "iopub.status.idle": "2024-07-04T09:41:08.464012Z",
     "shell.execute_reply": "2024-07-04T09:41:08.462846Z"
    },
    "papermill": {
     "duration": 2.41415,
     "end_time": "2024-07-04T09:41:08.466824",
     "exception": false,
     "start_time": "2024-07-04T09:41:06.052674",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lemmatized words of the list are : \t ['Organization', 'Organizational', 'Factual']\n"
     ]
    }
   ],
   "source": [
    "wl=WordNetLemmatizer()\n",
    "\n",
    "#   list item lemmitization\n",
    "lemma_li=[]\n",
    "for i in words: \n",
    "    lemma_li.append(wl.lemmatize(i))\n",
    "    \n",
    "print(\"The lemmatized words of the list are : \\t\",lemma_li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa1ad411",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-04T09:41:08.482354Z",
     "iopub.status.busy": "2024-07-04T09:41:08.481931Z",
     "iopub.status.idle": "2024-07-04T09:41:08.489297Z",
     "shell.execute_reply": "2024-07-04T09:41:08.487946Z"
    },
    "papermill": {
     "duration": 0.018124,
     "end_time": "2024-07-04T09:41:08.491808",
     "exception": false,
     "start_time": "2024-07-04T09:41:08.473684",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lemmatized words of the sentence words tokens will be : \n",
      " ['He', 'stepped', 'gingerly', 'onto', 'the', 'bridge', 'knowing', ',', 'that', 'enchantment', 'awaited', 'on', 'the', 'other', 'side', '.', 'It', 'wa', 'difficult', 'for', 'Mary', 'to', 'admit', ';', 'that', 'most', 'of', 'her', 'workout', 'consisted', 'of', 'exercising', 'poor', 'judgment', '.']\n"
     ]
    }
   ],
   "source": [
    "lemma_words = []\n",
    "for i in tokens:\n",
    "    lemma_words.append(wl.lemmatize(i))\n",
    "    \n",
    "print(\"The lemmatized words of the sentence words tokens will be : \\n\",lemma_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d2aadcd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-04T09:41:08.507832Z",
     "iopub.status.busy": "2024-07-04T09:41:08.507454Z",
     "iopub.status.idle": "2024-07-04T09:41:08.514164Z",
     "shell.execute_reply": "2024-07-04T09:41:08.512751Z"
    },
    "papermill": {
     "duration": 0.018372,
     "end_time": "2024-07-04T09:41:08.517426",
     "exception": false,
     "start_time": "2024-07-04T09:41:08.499054",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lemmatized sentence will be of the paragraph is : \n",
      " ['He stepped gingerly onto the bridge knowing, that enchantment awaited on the other side.', 'It was difficult for Mary to admit; that most of her workout consisted of exercising poor judgment.']\n"
     ]
    }
   ],
   "source": [
    "lemma_sentence = []\n",
    "for i in tokens_sent:\n",
    "    lemma_sentence.append(wl.lemmatize(i))\n",
    "    \n",
    "print(\"The lemmatized sentence will be of the paragraph is : \\n\",lemma_sentence)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 12.068842,
   "end_time": "2024-07-04T09:41:09.447224",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-04T09:40:57.378382",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
